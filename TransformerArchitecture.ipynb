{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOtKWQLuqJtPVNzetaFD/As",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mgaac/ml_repertoire/blob/main/TransformerArchitecture.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "FnzTjjVx5-9o"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as f\n",
        "\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Multi_head_attention(nn.Module):\n",
        "  def __init__(self, embed_size, n_heads):\n",
        "    super(Multi_head_attention, self).__init__()\n",
        "\n",
        "    self.embed_size = embed_size\n",
        "    self.n_heads = n_heads\n",
        "    self.head_dim = embed_size // n_heads\n",
        "\n",
        "    assert(self.head_dim * n_heads == embed_size)\n",
        "\n",
        "    self.query = nn.Linear(embed_size, embed_size)\n",
        "    self.key = nn.Linear(embed_size, embed_size)\n",
        "    self.value = nn.Linear(embed_size, embed_size)\n",
        "\n",
        "  def forward(self, x, value=None, query=None, key=None, mask=None):\n",
        "    embed_size = self.embed_size \n",
        "    head_dim = self.head_dim\n",
        "    n_heads = self.n_heads\n",
        "\n",
        "    if None in [query, key, value]:\n",
        "      query = self.query(x)\n",
        "      key = self.key(x)\n",
        "      value = self.value(x)\n",
        "    \n",
        "    else:\n",
        "      query = self.query(query)\n",
        "      key = self.key(key)\n",
        "      value = self.value(value)\n",
        "\n",
        "\n",
        "    query = query.reshape(x.size()[0], head_dim, n_heads)\n",
        "    key = key.reshape(x.size()[0], head_dim, n_heads)\n",
        "    value = value.reshape(x.size()[0], head_dim, n_heads)\n",
        "\n",
        "    alingment = torch.matmul(torch.transpose(key, 1, 2), query)[:,0] # s_size, n_heads\n",
        "    scaled_alingment = torch.div(alingment, (embed_size ** .5))\n",
        "\n",
        "    if mask is not None:\n",
        "      scaled_alingment = scaled_alingment.masked_fill(\n",
        "          mask == 0, float(1e-20))\n",
        "\n",
        "    weights = f.softmax(scaled_alingment, 1) \n",
        "    weighted_values = torch.einsum('sij,sj->sij', [value,weights]) # (s_size, e_size, n_heads) * (s_size, n_heads)\n",
        "    weighted_values = weighted_values.reshape(\n",
        "        weighted_values.size()[0], head_dim * n_heads)\n",
        "\n",
        "    return weighted_values"
      ],
      "metadata": {
        "id": "yRh-gKtI6LzA"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_mask(input, mask_idx):\n",
        "  mask = torch.zeros(input.size())\n",
        "  for x in range(mask_idx):\n",
        "    idx = torch.tensor([x])\n",
        "    mask = mask.index_fill(2, idx, 1)\n",
        "\n",
        "  return mask"
      ],
      "metadata": {
        "id": "celp_Eq2a2MY"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, embed_size, n_heads, ff_dim):\n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    self.embed_size = embed_size\n",
        "    self.n_heads = n_heads\n",
        "    self.ff_dim = ff_dim\n",
        "\n",
        "    self.feed_forward = nn.Sequential(\n",
        "        nn.Linear(embed_size, ff_dim),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(ff_dim, embed_size))\n",
        "  \n",
        "    self.attention = Multi_head_attention(embed_size, n_heads)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    n_heads = self.n_heads\n",
        "\n",
        "    att = self.attention(x)\n",
        "    res1 = torch.add(att, x)  # Add residual conneciton to each head independently\n",
        "    norm1 = f.layer_norm(res1, res1.size())   \n",
        "\n",
        "    ff = self.feed_forward(norm1)\n",
        "    res2 = torch.add(ff, norm1)\n",
        "    norm2 = f.layer_norm(res2, res2.size())\n",
        "\n",
        "    return norm2"
      ],
      "metadata": {
        "id": "KLic4IU08PHO"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, embed_size, n_heads, ff_dim):\n",
        "    super (Decoder, self).__init__()\n",
        "\n",
        "    self.embed_size = embed_size       \n",
        "    self.n_heads = n_heads\n",
        "    self.ff_dim = ff_dim\n",
        "    \n",
        "    self.feed_forward = nn.Sequential(\n",
        "        nn.Linear(embed_size, ff_dim),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(ff_dim, embed_size))\n",
        "  \n",
        "    self.attention = Multi_head_attention(embed_size, n_heads)\n",
        "    \n",
        "  def forward(self, x, query, key, value=None, mask=None):\n",
        "    n_heads = self.n_heads\n",
        "    embed_size = self.embed_size\n",
        "    \n",
        "    att1 = self.attention(x, mask)\n",
        "    res1 = torch.add(att1, x)  \n",
        "    norm1 = f.layer_norm(res1, res1.size())\n",
        "\n",
        "\n",
        "    att2 = self.attention(norm1, query, key, norm1)\n",
        "\n",
        "    if value is not None:\n",
        "      att2 = self.attention(norm1, query, key, value)\n",
        "      \n",
        "    res2 = torch.add(att2, norm1)\n",
        "    norm2 = f.layer_norm(res2, res2.size())\n",
        "\n",
        "    ff = self.feed_forward(norm2)\n",
        "    res3 = torch.add(ff, norm2)\n",
        "    norm3 = f.layer_norm(res3, res3.size())\n",
        "\n",
        "    return norm3\n"
      ],
      "metadata": {
        "id": "RU8djbzbmGBU"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Embedding(nn.Module):\n",
        "  def __init__(self, input_size, embed_size, ff_dim):\n",
        "    super (Embedding, self).__init__()\n",
        "\n",
        "    self.sequential = nn.Sequential(\n",
        "        nn.Linear(input_size, embed_size),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(embed_size, embed_size),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(embed_size, embed_size))\n",
        "    \n",
        "  def forward(self, x):\n",
        "    x = self.sequential(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "y21YL1Ev-vHy"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "  def __init__(self, input_size, embed_size, n_heads, n_encoder, n_decoder, ff_dim, out_dim):\n",
        "    super(Transformer, self).__init__()\n",
        "\n",
        "    self.input_size = input_size\n",
        "    self.embed_size = embed_size\n",
        "    self.n_heads = n_heads\n",
        "    self.ff_dim = ff_dim\n",
        "    self.n_encoder = n_encoder\n",
        "    self.n_decoder = n_decoder\n",
        "\n",
        "    encoder = Encoder(embed_size, n_heads, ff_dim)\n",
        "    decoder = Decoder(embed_size, n_heads, ff_dim)\n",
        "\n",
        "    self.encoder_block = nn.ModuleList([encoder for i in range(n_encoder)])\n",
        "    self.decoder_block = nn.ModuleList([decoder for i in range(n_decoder)])\n",
        "\n",
        "    self.ff = nn.Sequential(\n",
        "        nn.Linear(embed_size, ff_dim),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(ff_dim, ff_dim),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(ff_dim, ff_dim),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(ff_dim, out_dim))\n",
        "    \n",
        "  def forward(self, encoder_var, decoder_var):\n",
        "    for submodule in enumerate(self.encoder_block):\n",
        "      encoder_var = submodule[1](encoder_var)\n",
        "    \n",
        "    for submodule in enumerate(self.decoder_block):\n",
        "      decoder_var = submodule[1](decoder_var, encoder_var, encoder_var)\n",
        "\n",
        "    out = self.ff(decoder_var)\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "IFN2RBHIAU7i"
      },
      "execution_count": 99,
      "outputs": []
    }
  ]
}