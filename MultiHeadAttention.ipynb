{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMRV8EYGsyj1I5/s/wJvaWJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mgaac/ml_repertoire/blob/main/MultiHeadAttention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJUcrzmkMJ4I"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as f\n",
        "\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Multi_head_attention(nn.Module):\n",
        "  def __init__(self, embed_size, n_heads):\n",
        "    super(Multi_head_attention, self).__init__()\n",
        "\n",
        "    self.embed_size = embed_size\n",
        "    self.n_heads = n_heads\n",
        "    self.head_dim = embed_size // n_heads\n",
        "\n",
        "    assert(self.head_dim * n_heads == embed_size)\n",
        "\n",
        "    self.query = nn.Linear(embed_size, embed_size)\n",
        "    self.key = nn.Linear(embed_size, embed_size)\n",
        "    self.value = nn.Linear(embed_size, embed_size)\n",
        "\n",
        "  def forward(self, x, value=None, query=None, key=None, mask=None):\n",
        "    embed_size = self.embed_size \n",
        "    head_dim = self.head_dim\n",
        "    n_heads = self.n_heads\n",
        "\n",
        "    if None in [query, key, value]:\n",
        "      query = self.query(x)\n",
        "      key = self.key(x)\n",
        "      value = self.value(x)\n",
        "    \n",
        "    else:\n",
        "      query = self.query(query)\n",
        "      key = self.key(key)\n",
        "      value = self.value(value)\n",
        "\n",
        "\n",
        "    query = query.reshape(x.size()[0], head_dim, n_heads)\n",
        "    key = key.reshape(x.size()[0], head_dim, n_heads)\n",
        "    value = value.reshape(x.size()[0], head_dim, n_heads)\n",
        "\n",
        "    alingment = torch.matmul(torch.transpose(key, 1, 2), query)[:,0]\n",
        "    scaled_alingment = torch.div(alingment, (embed_size ** .5))\n",
        "\n",
        "    if mask is not None:\n",
        "      scaled_alingment = scaled_alingment.masked_fill(\n",
        "          mask == 0, float(1e-20))\n",
        "\n",
        "    weights = f.softmax(scaled_alingment, 1) \n",
        "    weighted_values = torch.einsum('sij,sj->sij', [value,weights])\n",
        "    weighted_values = weighted_values.reshape(\n",
        "        weighted_values.size()[0], head_dim * n_heads)\n",
        "\n",
        "    return weighted_values"
      ],
      "metadata": {
        "id": "IcSR0xA6MRpc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}